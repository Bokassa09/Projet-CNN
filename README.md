# ü´Å D√©tection de Pneumonie par Deep Learning

##  Description
Ce projet utilise un r√©seau de neurones convolutionnels (CNN) pour d√©tecter automatiquement la pneumonie sur des radiographies thoraciques.

##  Objectif
Classifier les radiographies en deux cat√©gories :
- **NORMAL** : Poumons sains
- **PNEUMONIA** : Pneumonie d√©tect√©e

##  Dataset
- **Source** : [Kaggle - Chest X-Ray Images (Pneumonia)](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)
- **Taille** : 1.15 GB
- **Format** : Images JPEG en niveaux de gris

### R√©partition des donn√©es
- **Train** : 1,341 NORMAL + 3,875 PNEUMONIA
- **Test** : 234 NORMAL + 390 PNEUMONIA
- **Validation** : 8 NORMAL + 8 PNEUMONIA

##  Architecture du mod√®le
- **Type** : CNN (Convolutional Neural Network)
- **Input** : Images 150x150 pixels (grayscale)
- **Couches** : 3 blocs convolutionnels + couche dense
- **Activation finale** : Sigmoid (classification binaire)

##  Technologies utilis√©es
- Python 3.x
- TensorFlow / Keras
- NumPy
- Matplotlib
- Scikit-learn

##  Installation
```bash
# Cr√©er l'environnement virtuel
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer les d√©pendances
pip install -r requirements.txt

# Entra√Æner le mod√®le
python main.py

# Conclusion

Dans le fichier texte Theorie.txt, je pr√©sente les approches et les modifications qui m‚Äôont permis d‚Äôam√©liorer les performances de mon mod√®le.

Dans ce projet, j‚Äôai entra√Æn√© un r√©seau de neurones convolutif (CNN) avec pour objectif d‚Äôobtenir un mod√®le moins gourmand en ressources ‚Äî notamment en puissance de calcul et en m√©moire (RAM).
Gr√¢ce √† plusieurs optimisations d√©taill√©es dans ce document, j‚Äôai r√©ussi √† am√©liorer la pr√©cision (accuracy) du mod√®le, passant de 78% √† environ 89 %....91 %.

Les quatre √©l√©ments qui ont eu le plus d‚Äôimpact sur cette am√©lioration, et qu‚Äôil serait pertinent de conserver pour les futurs projets, sont :

1. Le choix appropri√© du learning rate ;

2. L‚Äôajustement de la taille du batch size ;

3. L‚Äôutilisation des class weights ;

4. Le nombre de neurones dans la derni√®re couche de classification (couche enti√®rement connect√©e).

Dans ce projet, certains participants ont utilis√© des mod√®les contenant jusqu‚Äô√† 14 millions de
param√®tres pour atteindre une telle performance, d‚Äôautres environ 1,5 million, et moi 3,7 millions.
D‚Äôautres encore ont atteint une tr√®s haute pr√©cision avec 28 millions de param√®tres.

Ce projet, h√©berg√© sur Kaggle, a surtout √©t√© pour moi une occasion d‚Äôapprendre l‚Äôing√©nierie de l‚Äôentra√Ænement
d‚Äôun mod√®le : comprendre comment chaque choix influence les performances est la cl√© pour progresser dans ce domaine.

Auteur: BOUEKE Omer Bokassa 
HPC&IA Ingenieur
